{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "eeg_raw = pd.read_csv(os.path.join(\"data\",\"eeg-data.csv\"))\n",
    "eeg_raw['eeg_power'] = eeg_raw.eeg_power.map(json.loads)\n",
    "stimulus_times = pd.read_csv(os.path.join(\"data\",\"stimulus-times.csv\"))\n",
    "metadata = pd.read_csv(os.path.join(\"data\", \"subject-metadata.csv\"))\n",
    "\n",
    "# drop the rows with signal quality greater than 128, as that indicates the headset is not worn properly\n",
    "idx = eeg_raw.signal_quality <= 128\n",
    "eeg = eeg_raw[idx]\n",
    "\n",
    "# drop the unlabeled ys as those are useless\n",
    "idx = eeg.label != 'unlabeled'\n",
    "eeg = eeg[idx]\n",
    "\n",
    "# clean up the data\n",
    "y = eeg.label\n",
    "eeg = eeg.drop([\"Unnamed: 0\", \"indra_time\",\"browser_latency\", \"reading_time\",\"createdAt\",\"updatedAt\", \"attention_esense\", \"meditation_esense\",\"raw_values\",\"signal_quality\",\"label\"], axis=1)\n",
    "\n",
    "# split the eeg readings into columns\n",
    "eeg[['eeg1','eeg2','eeg3','eeg4','eeg5','eeg6','eeg7', 'eeg8']] = pd.DataFrame(eeg.eeg_power.values.tolist(), index= eeg.index)\n",
    "\n",
    "# drop the eeg power\n",
    "eeg = eeg.drop(['eeg_power'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the metadata\n",
    "metadata = metadata.drop([\"Session\",\"Saw icons?\",\"Chosen color\"],axis=1)\n",
    "metadata.columns = [\"id\",\"seen_video\",\"gender\",\"wear_contacts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the subject metadata\n",
    "eeg = eeg.merge(metadata, on=\"id\")\n",
    "\n",
    "# drop the id\n",
    "eeg = eeg.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "eeg.seen_video = (eeg.seen_video == 'y') * 1.0\n",
    "eeg.wear_contacts = (eeg.wear_contacts == 'y') * 1.0\n",
    "eeg = pd.get_dummies(eeg, columns=[\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the labels so that we use the general task rather than the instance of that task\n",
    "np.unique(y)\n",
    "map_dict = {\n",
    "    'blinkInstruction':'instruction',\n",
    "    'blink1':'blink',\n",
    "    'blink2':'blink',\n",
    "    'blink3':'blink',\n",
    "    'blink4':'blink',\n",
    "    'blink5':'blink',\n",
    "    'colorInstruction1':'instruction',\n",
    "    'colorInstruction2':'instruction',\n",
    "    'colorRound1-1':'colorRound',\n",
    "    'colorRound1-2':'colorRound',\n",
    "    'colorRound1-3':'colorRound',\n",
    "    'colorRound1-4':'colorRound',\n",
    "    'colorRound1-5':'colorRound',\n",
    "    'colorRound1-6':'colorRound',\n",
    "    'colorRound2-1':'colorRound',\n",
    "    'colorRound2-2':'colorRound',\n",
    "    'colorRound2-3':'colorRound',\n",
    "    'colorRound2-4':'colorRound',\n",
    "    'colorRound2-5':'colorRound',\n",
    "    'colorRound2-6':'colorRound',\n",
    "    'colorRound3-1':'colorRound',\n",
    "    'colorRound3-2':'colorRound',\n",
    "    'colorRound3-3':'colorRound',\n",
    "    'colorRound3-4':'colorRound',\n",
    "    'colorRound3-5':'colorRound',\n",
    "    'colorRound3-6':'colorRound',\n",
    "    'colorRound4-1':'colorRound',\n",
    "    'colorRound4-2':'colorRound',\n",
    "    'colorRound4-3':'colorRound',\n",
    "    'colorRound4-4':'colorRound',\n",
    "    'colorRound4-5':'colorRound',\n",
    "    'colorRound4-6':'colorRound',\n",
    "    'colorRound5-1':'colorRound',\n",
    "    'colorRound5-2':'colorRound',\n",
    "    'colorRound5-3':'colorRound',\n",
    "    'colorRound5-4':'colorRound',\n",
    "    'colorRound5-5':'colorRound',\n",
    "    'colorRound5-6':'colorRound',\n",
    "    'colorRound6-1':'colorRound',\n",
    "    'colorRound6-2':'colorRound',\n",
    "    'colorRound6-3':'colorRound',\n",
    "    'colorRound6-4':'colorRound',\n",
    "    'colorRound6-5':'colorRound',\n",
    "    'colorRound6-6':'colorRound',\n",
    "    'math1':'math',\n",
    "    'math2':'math',\n",
    "    'math3':'math',\n",
    "    'math4':'math',\n",
    "    'math5':'math',\n",
    "    'math6':'math',\n",
    "    'math7':'math',\n",
    "    'math8':'math',\n",
    "    'math9':'math',\n",
    "    'math10':'math',\n",
    "    'math11':'math',\n",
    "    'math12':'math',\n",
    "    'readyRound1':'readyRound',\n",
    "    'readyRound2':'readyRound',\n",
    "    'readyRound3':'readyRound',\n",
    "    'readyRound4':'readyRound',\n",
    "    'readyRound5':'readyRound',\n",
    "    'thinkOfItems-ver1':'thinkOfItems',\n",
    "    'thinkOfItems-ver2':'thinkOfItems',\n",
    "    'thinkOfItemsInstruction-ver1':'instruction',\n",
    "    'thinkOfItemsInstruction-ver2':'instruction',\n",
    "    'video-ver1':'video',\n",
    "    'video-ver2':'video',\n",
    "    'relaxInstruction':'instruction',\n",
    "    'videoInstruction':'instruction',\n",
    "    'mathInstruction':'instruction',\n",
    "    'musicInstruction':'instruction',\n",
    "}\n",
    "new_y = np.copy(y)\n",
    "for k, v in map_dict.items(): new_y[y==k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove instructions\n",
    "instr_idx = new_y != 'instruction'\n",
    "\n",
    "new_y = new_y[instr_idx]\n",
    "eeg = eeg[instr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr (5839, 12)\n",
      "X_cv (974, 12)\n",
      "X_te (973, 12)\n",
      "y_tr (5839,)\n",
      "y_cv (974,)\n",
      "y_te (973,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and validation\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(eeg, new_y, test_size=0.25, random_state=1)\n",
    "\n",
    "# split test into test and validation\n",
    "X_te, X_cv, y_te, y_cv = train_test_split(X_te, y_te, test_size=0.5, random_state=1)\n",
    "\n",
    "print(\"X_tr\", X_tr.shape)\n",
    "print(\"X_cv\", X_cv.shape)\n",
    "print(\"X_te\", X_te.shape)\n",
    "print(\"y_tr\", y_tr.shape)\n",
    "print(\"y_cv\", y_cv.shape)\n",
    "print(\"y_te\", y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try to classify with an SVM as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scaler', None), ('svm', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'svm__C': [0.125, 0.25, 0.375, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', None),\n",
    "    ('svm', LinearSVC())\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'svm__C': [0.125, 0.25, 0.375, 0.5],\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(pipe, grid, cv=3)\n",
    "\n",
    "grid_cv.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.241993</td>\n",
       "      <td>0.070066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.218017</td>\n",
       "      <td>0.050206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.198835</td>\n",
       "      <td>0.077987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.164754</td>\n",
       "      <td>0.028465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C  mean_accuracy       std\n",
       "0  0.125       0.241993  0.070066\n",
       "3    0.5       0.218017  0.050206\n",
       "2  0.375       0.198835  0.077987\n",
       "1   0.25       0.164754  0.028465"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect results and sort them\n",
    "df = pd.DataFrame.from_items([\n",
    "    ('C', grid_cv.cv_results_['param_svm__C']),\n",
    "    ('mean_accuracy', grid_cv.cv_results_['mean_test_score']),\n",
    "    ('std', grid_cv.cv_results_['std_test_score'])\n",
    "])\n",
    "\n",
    "df.sort_values(by='mean_accuracy', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Not very good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform labels into integers\n",
    "names = np.unique(new_y)\n",
    "i = 0\n",
    "for name in names:\n",
    "    new_y[new_y == name] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr (5839, 12)\n",
      "X_cv (974, 12)\n",
      "X_te (973, 12)\n",
      "y_tr (5839,)\n",
      "y_cv (974,)\n",
      "y_te (973,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and validation\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(eeg.values, new_y, test_size=0.25, random_state=1)\n",
    "\n",
    "# split test into test and validation\n",
    "X_te, X_cv, y_te, y_cv = train_test_split(X_te, y_te, test_size=0.5, random_state=1)\n",
    "\n",
    "print(\"X_tr\", X_tr.shape)\n",
    "print(\"X_cv\", X_cv.shape)\n",
    "print(\"X_te\", X_te.shape)\n",
    "print(\"y_tr\", y_tr.shape)\n",
    "print(\"y_cv\", y_cv.shape)\n",
    "print(\"y_te\", y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "model_name = 'model_1'\n",
    "\n",
    "with graph.as_default():\n",
    "    # Create placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 12])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # create variables\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # create learning rate\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                            0.01, \n",
    "                            global_step, \n",
    "                            100,     \n",
    "                            0.95,    \n",
    "                            staircase=True)\n",
    "    \n",
    "    # hidden layer 1\n",
    "    hidden1 = tf.layers.dense(\n",
    "        X,                              # input\n",
    "        1024,                             # 64 units\n",
    "        activation=tf.nn.relu,          # activation\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),  # kernel initializer\n",
    "        bias_initializer=tf.zeros_initializer(), # bias\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "        name='hidden1'                   # name\n",
    "    )\n",
    "    \n",
    "    hidden2 = tf.layers.dense(\n",
    "        hidden1,                         # input\n",
    "        56,                             # 48 units\n",
    "        activation=tf.nn.relu,          # activation\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),  # kernel initializer\n",
    "        bias_initializer=tf.zeros_initializer(), # bias\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "        name='hidden2'                  # name\n",
    "    )\n",
    "    \n",
    "    logits = tf.layers.dense(\n",
    "        hidden2,                       # input\n",
    "        8,                             # 4 units\n",
    "        activation=None,               # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "        name='output'\n",
    "    )\n",
    "    \n",
    "    # cross entropy and loss\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "    \n",
    "    gd = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Minimize loss\n",
    "    train_op = gd.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    \n",
    "    # extra ops if we add batch norm\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', mean_ce)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CONFIGURE OPTIONS\n",
    "init = True                   # whether to initialize the model or use a saved version\n",
    "\n",
    "meta_data_every = 5\n",
    "log_to_tensorboard = True\n",
    "print_every = 3                # how often to print metrics\n",
    "checkpoint_every = 1           # how often to save model in epochs\n",
    "use_gpu = False                 # whether or not to use the GPU\n",
    "print_metrics = False          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
    "epochs = 1000\n",
    "batch_size = 256\n",
    "\n",
    "# Placeholders for metrics\n",
    "if init:\n",
    "    valid_acc_values = []\n",
    "    valid_cost_values = []\n",
    "    train_acc_values = []\n",
    "    train_cost_values = []\n",
    "    train_lr_values = []\n",
    "    train_loss_values = []\n",
    "\n",
    "if use_gpu:\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allocator_type = 'BFC'\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "else:\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    if log_to_tensorboard:\n",
    "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
    "    \n",
    "    if not print_metrics:\n",
    "        # create a plot to be updated as model is trained\n",
    "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
    "    \n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # If the model is new initialize variables, else restore the session\n",
    "    if init:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    print(\"Training\", model_name, \"...\")\n",
    "    \n",
    "    # Train several epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "        batch_cost = []\n",
    "        batch_loss = []\n",
    "        batch_lr = []\n",
    "        \n",
    "        # only log run metadata once per epoch\n",
    "        write_meta_data = False\n",
    "            \n",
    "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size):\n",
    "            if write_meta_data and log_to_tensboard:\n",
    "                # create the metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "            \n",
    "                # Run training and evaluate accuracy\n",
    "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
    "                    X: X_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                },\n",
    "                options=run_options,\n",
    "                run_metadata=run_metadata)\n",
    "\n",
    "                # Save accuracy (current batch)\n",
    "                batch_acc.append(acc_value)\n",
    "                batch_cost.append(cost_value)\n",
    "                batch_lr.append(lr)\n",
    "                batch_loss.append(loss_value)\n",
    "  \n",
    "                # write the summary\n",
    "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                write_meta_data = False\n",
    "                \n",
    "            else:\n",
    "                # Run training without meta data\n",
    "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
    "                    X: X_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "\n",
    "                # Save accuracy (current batch)\n",
    "                batch_acc.append(acc_value)\n",
    "                batch_cost.append(cost_value)\n",
    "                batch_lr.append(lr)\n",
    "                batch_loss.append(loss_value)\n",
    "                \n",
    "                # write the summary\n",
    "                if log_to_tensorboard:\n",
    "                    train_writer.add_summary(summary, step)\n",
    "\n",
    "        # save checkpoint every nth epoch\n",
    "        if(epoch % checkpoint_every == 0):\n",
    "            print(\"Saving checkpoint\")\n",
    "            # save the model\n",
    "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
    "    \n",
    "            # Now that model is saved set init to false so we reload it\n",
    "            init = False\n",
    "        \n",
    "        # init batch arrays\n",
    "        batch_cv_acc = []\n",
    "        batch_cv_cost = []\n",
    "        batch_cv_loss = []\n",
    "        \n",
    "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
    "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size):\n",
    "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "            batch_cv_acc.append(valid_acc)\n",
    "            batch_cv_cost.append(valid_cost)\n",
    "            batch_cv_loss.append(valid_loss)\n",
    "\n",
    "        # Write average of validation data to summary logs\n",
    "        if log_to_tensorboard:\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
    "            test_writer.add_summary(summary, step)\n",
    "            step += 1\n",
    "            \n",
    "        # take the mean of the values to add to the metrics\n",
    "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
    "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
    "        train_acc_values.append(np.mean(batch_acc))\n",
    "        train_cost_values.append(np.mean(batch_cost))\n",
    "        train_lr_values.append(np.mean(batch_lr))\n",
    "        train_loss_values.append(np.mean(batch_loss))\n",
    "        \n",
    "        if print_metrics:\n",
    "            # Print progress every nth epoch to keep output to reasonable amount\n",
    "            if(epoch % print_every == 0):\n",
    "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
    "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
    "                ))\n",
    "        else:\n",
    "            # update the plot\n",
    "            ax[0].cla()\n",
    "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
    "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
    "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
    "            \n",
    "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
    "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
    "                ax[0].set_ylim([0.75,1.0])\n",
    "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
    "                ax[0].set_ylim([0.65,1.0])\n",
    "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
    "                ax[0].set_ylim([0.55,1.0])\n",
    "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
    "                ax[0].set_ylim([0.45,1.0])           \n",
    "            \n",
    "            ax[0].set_xlabel('Epoch')\n",
    "            ax[0].set_ylabel('Accuracy')\n",
    "            ax[0].legend()\n",
    "            \n",
    "            ax[1].cla()\n",
    "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
    "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
    "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
    "            ax[1].set_xlabel('Epoch')\n",
    "            ax[1].set_ylabel('Cross Entropy')\n",
    "            ax[1].legend()\n",
    "            \n",
    "            ax[2].cla()\n",
    "            ax[2].plot(train_lr_values)\n",
    "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
    "            ax[2].set_xlabel(\"Epoch\")\n",
    "            ax[2].set_ylabel(\"Learning Rate\")\n",
    "            \n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "        # Print data every 50th epoch so I can write it down to compare models\n",
    "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
    "            if(epoch % print_every == 0):\n",
    "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
    "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
    "                ))  \n",
    "            \n",
    "    # print results of last epoch\n",
    "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
    "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
    "            ))\n",
    "    \n",
    "    # save the session\n",
    "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
    "    \n",
    "    # init the test data array\n",
    "    test_acc_values = []\n",
    "    \n",
    "    # Check on the test data\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size):\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "        test_acc_values.append(test_accuracy)\n",
    "    \n",
    "    # average test accuracy across batches\n",
    "    test_acc = np.mean(test_acc_values)\n",
    "    \n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "# print results of last epoch\n",
    "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
    "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
    "    ))\n",
    "    \n",
    "# print test accuracy\n",
    "#print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation set:\", valid_acc_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blink', 'blinkInstruction', 'colorInstruction', 'colorRound',\n",
       "       'math', 'mathInstruction', 'music', 'musicInstruction',\n",
       "       'readyRound', 'relax', 'relaxInstruction', 'thinkOfItems',\n",
       "       'thinkOfItemsInstruction', 'video', 'videoInstruction'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
